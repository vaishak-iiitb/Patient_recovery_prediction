{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fc95cc",
   "metadata": {},
   "source": [
    "# Patient Recovery Prediction - XGBoost Model\n",
    "## Advanced Gradient Boosting Approach\n",
    "\n",
    "**Project Deadline:** October 26th, 11:55 P.M.\n",
    "\n",
    "**Model:** XGBoost Regressor with Hyperparameter Tuning\n",
    "\n",
    "**Objective:** Leverage gradient boosting for improved predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6209ebb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ebfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBRegressor\n",
    "    print(\"‚úÖ XGBoost imported successfully!\")\n",
    "    print(f\"   XGBoost version: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå XGBoost not found. Installing...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install xgboost\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBRegressor\n",
    "    print(\"‚úÖ XGBoost installed and imported!\")\n",
    "\n",
    "# Machine Learning - Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9a94b",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee119e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nFeatures: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data summary\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nMissing values:\\n{train_df.isnull().sum()}\")\n",
    "print(f\"\\nDuplicate rows: {train_df.duplicated().sum()}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313f53",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = train_df.drop(['Id', 'Recovery Index'], axis=1).copy()\n",
    "y = train_df['Recovery Index'].copy()\n",
    "\n",
    "# Encode categorical variable\n",
    "label_encoder = LabelEncoder()\n",
    "X['Lifestyle Activities'] = label_encoder.fit_transform(X['Lifestyle Activities'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Lifestyle Activities encoded (No=0, Yes=1)\")\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {X.columns.tolist()}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std: {y.std():.2f}\")\n",
    "print(f\"  Range: [{y.min()}, {y.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a912ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN-VALIDATION SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({(X_train.shape[0]/X.shape[0])*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({(X_val.shape[0]/X.shape[0])*100:.1f}%)\")\n",
    "print(f\"\\nTraining target statistics:\")\n",
    "print(f\"  Mean: {y_train.mean():.2f}\")\n",
    "print(f\"  Std: {y_train.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8075c9bf",
   "metadata": {},
   "source": [
    "## 4. Baseline XGBoost Model (Default Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b151f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline XGBoost with default parameters\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE XGBOOST MODEL (Default Parameters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "xgb_baseline = XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgb_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_baseline = xgb_baseline.predict(X_train)\n",
    "y_val_pred_baseline = xgb_baseline.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2_baseline = r2_score(y_train, y_train_pred_baseline)\n",
    "val_r2_baseline = r2_score(y_val, y_val_pred_baseline)\n",
    "train_rmse_baseline = np.sqrt(mean_squared_error(y_train, y_train_pred_baseline))\n",
    "val_rmse_baseline = np.sqrt(mean_squared_error(y_val, y_val_pred_baseline))\n",
    "train_mae_baseline = mean_absolute_error(y_train, y_train_pred_baseline)\n",
    "val_mae_baseline = mean_absolute_error(y_val, y_val_pred_baseline)\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'R¬≤ Score':<20} {train_r2_baseline:>15.4f} {val_r2_baseline:>15.4f}\")\n",
    "print(f\"{'RMSE':<20} {train_rmse_baseline:>15.4f} {val_rmse_baseline:>15.4f}\")\n",
    "print(f\"{'MAE':<20} {train_mae_baseline:>15.4f} {val_mae_baseline:>15.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting_diff = abs(train_r2_baseline - val_r2_baseline)\n",
    "if overfitting_diff < 0.05:\n",
    "    print(f\"\\n‚úÖ Model is well-balanced (R¬≤ difference: {overfitting_diff:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Potential overfitting detected (R¬≤ difference: {overfitting_diff:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df16d39",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956669b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance_baseline = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_baseline.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (Baseline XGBoost)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance_baseline)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_baseline['Feature'], feature_importance_baseline['Importance'], color='teal')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance - XGBoost (Default)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76755ff",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning with RandomizedSearchCV\n",
    "\n",
    "Using RandomizedSearchCV for efficiency with large parameter spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c980bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distribution for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [0.1, 1, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING WITH RANDOMIZED SEARCH\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nParameter distributions:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nRandomized search will test 100 random combinations\")\n",
    "print(f\"This is more efficient than testing all combinations!\")\n",
    "print(\"\\nüîÑ Starting Randomized Search (this may take several minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOMIZED SEARCH RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Randomized Search completed!\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation R¬≤ score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 parameter combinations\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "top_10 = cv_results.nlargest(10, 'mean_test_score')[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 10 PARAMETER COMBINATIONS\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"\\nRank {int(row['rank_test_score'])}:\")\n",
    "    print(f\"  Mean R¬≤ Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']:.4f})\")\n",
    "    print(f\"  Parameters: {row['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1125dc9",
   "metadata": {},
   "source": [
    "## 7. Evaluate Tuned XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e547b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "xgb_tuned = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_tuned = xgb_tuned.predict(X_train)\n",
    "y_val_pred_tuned = xgb_tuned.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2_tuned = r2_score(y_train, y_train_pred_tuned)\n",
    "val_r2_tuned = r2_score(y_val, y_val_pred_tuned)\n",
    "train_rmse_tuned = np.sqrt(mean_squared_error(y_train, y_train_pred_tuned))\n",
    "val_rmse_tuned = np.sqrt(mean_squared_error(y_val, y_val_pred_tuned))\n",
    "train_mae_tuned = mean_absolute_error(y_train, y_train_pred_tuned)\n",
    "val_mae_tuned = mean_absolute_error(y_val, y_val_pred_tuned)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TUNED XGBOOST MODEL PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Metric':<20} {'Training':>15} {'Validation':>15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'R¬≤ Score':<20} {train_r2_tuned:>15.4f} {val_r2_tuned:>15.4f}\")\n",
    "print(f\"{'RMSE':<20} {train_rmse_tuned:>15.4f} {val_rmse_tuned:>15.4f}\")\n",
    "print(f\"{'MAE':<20} {train_mae_tuned:>15.4f} {val_mae_tuned:>15.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting_diff_tuned = abs(train_r2_tuned - val_r2_tuned)\n",
    "if overfitting_diff_tuned < 0.05:\n",
    "    print(f\"\\n‚úÖ Model is well-balanced (R¬≤ difference: {overfitting_diff_tuned:.4f})\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Potential overfitting detected (R¬≤ difference: {overfitting_diff_tuned:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd49a9",
   "metadata": {},
   "source": [
    "## 8. Model Comparison: Baseline vs Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['XGB Baseline', 'XGB Tuned'],\n",
    "    'Train R¬≤': [train_r2_baseline, train_r2_tuned],\n",
    "    'Val R¬≤': [val_r2_baseline, val_r2_tuned],\n",
    "    'Train RMSE': [train_rmse_baseline, train_rmse_tuned],\n",
    "    'Val RMSE': [val_rmse_baseline, val_rmse_tuned],\n",
    "    'Train MAE': [train_mae_baseline, train_mae_tuned],\n",
    "    'Val MAE': [val_mae_baseline, val_mae_tuned]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON: BASELINE vs TUNED\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Calculate improvement\n",
    "r2_improvement = ((val_r2_tuned - val_r2_baseline) / val_r2_baseline) * 100\n",
    "rmse_improvement = ((val_rmse_baseline - val_rmse_tuned) / val_rmse_baseline) * 100\n",
    "\n",
    "print(f\"\\nüìà Improvement with tuning:\")\n",
    "print(f\"  Validation R¬≤ improvement: {r2_improvement:+.2f}%\")\n",
    "print(f\"  Validation RMSE improvement: {rmse_improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "metrics = ['Val R¬≤', 'Val RMSE', 'Val MAE']\n",
    "baseline_vals = [val_r2_baseline, val_rmse_baseline, val_mae_baseline]\n",
    "tuned_vals = [val_r2_tuned, val_rmse_tuned, val_mae_tuned]\n",
    "\n",
    "for idx, (metric, baseline_val, tuned_val) in enumerate(zip(metrics, baseline_vals, tuned_vals)):\n",
    "    x = ['Baseline XGB', 'Tuned XGB']\n",
    "    y = [baseline_val, tuned_val]\n",
    "    colors = ['lightcoral', 'darkgreen']\n",
    "    \n",
    "    bars = axes[idx].bar(x, y, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    axes[idx].set_ylabel(metric, fontsize=11)\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{height:.4f}',\n",
    "                      ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c3b6a",
   "metadata": {},
   "source": [
    "## 9. Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be07daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for tuned model\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "\n",
    "# Train a new model with early stopping to get learning curves\n",
    "xgb_learning = XGBRegressor(**xgb_tuned.get_params())\n",
    "xgb_learning.set_params(early_stopping_rounds=20, eval_metric='rmse')\n",
    "\n",
    "xgb_learning.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Get results\n",
    "results = xgb_learning.evals_result()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['validation_0']['rmse'], label='Training RMSE', linewidth=2)\n",
    "plt.plot(results['validation_1']['rmse'], label='Validation RMSE', linewidth=2)\n",
    "plt.xlabel('Boosting Round', fontsize=11)\n",
    "plt.ylabel('RMSE', fontsize=11)\n",
    "plt.title('Learning Curve - RMSE', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "plt.plot(x_axis, results['validation_0']['rmse'], label='Training', linewidth=2, color='blue')\n",
    "plt.plot(x_axis, results['validation_1']['rmse'], label='Validation', linewidth=2, color='orange')\n",
    "plt.xlabel('Boosting Round', fontsize=11)\n",
    "plt.ylabel('RMSE', fontsize=11)\n",
    "plt.title('Convergence Analysis', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest iteration: {xgb_learning.best_iteration}\")\n",
    "print(f\"Best validation RMSE: {results['validation_1']['rmse'][xgb_learning.best_iteration]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef9876",
   "metadata": {},
   "source": [
    "## 10. Visualization: Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f31f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs Predicted for Tuned Model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred_tuned, alpha=0.5, s=30, color='blue', edgecolors='black', linewidth=0.3)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Recovery Index', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Recovery Index', fontsize=12)\n",
    "axes[0].set_title(f'Training Set: Actual vs Predicted (Tuned XGBoost)\\nR¬≤ = {train_r2_tuned:.4f}', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_val_pred_tuned, alpha=0.5, s=30, color='green', edgecolors='black', linewidth=0.3)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Recovery Index', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Recovery Index', fontsize=12)\n",
    "axes[1].set_title(f'Validation Set: Actual vs Predicted (Tuned XGBoost)\\nR¬≤ = {val_r2_tuned:.4f}', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25796527",
   "metadata": {},
   "source": [
    "## 11. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_train_tuned = y_train - y_train_pred_tuned\n",
    "residuals_val_tuned = y_val - y_val_pred_tuned\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training residuals vs predicted\n",
    "axes[0, 0].scatter(y_train_pred_tuned, residuals_train_tuned, alpha=0.5, s=30, \n",
    "                   color='blue', edgecolors='black', linewidth=0.3)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Recovery Index', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0, 0].set_title('Training Set: Residual Plot (Tuned XGBoost)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Validation residuals vs predicted\n",
    "axes[0, 1].scatter(y_val_pred_tuned, residuals_val_tuned, alpha=0.5, s=30, \n",
    "                   color='green', edgecolors='black', linewidth=0.3)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted Recovery Index', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0, 1].set_title('Validation Set: Residual Plot (Tuned XGBoost)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Histogram of training residuals\n",
    "axes[1, 0].hist(residuals_train_tuned, bins=50, color='blue', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 0].set_title('Training Set: Residual Distribution (Tuned XGBoost)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Histogram of validation residuals\n",
    "axes[1, 1].hist(residuals_val_tuned, bins=50, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Residuals', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Validation Set: Residual Distribution (Tuned XGBoost)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"RESIDUAL STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Mean residual: {residuals_train_tuned.mean():.4f}\")\n",
    "print(f\"  Std residual: {residuals_train_tuned.std():.4f}\")\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Mean residual: {residuals_val_tuned.mean():.4f}\")\n",
    "print(f\"  Std residual: {residuals_val_tuned.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdb224",
   "metadata": {},
   "source": [
    "## 12. Feature Importance (Tuned Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1900fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from tuned model\n",
    "feature_importance_tuned = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_tuned.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (Tuned XGBoost)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance_tuned)\n",
    "\n",
    "# Visualize feature importance (multiple types)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Weight importance\n",
    "xgb.plot_importance(xgb_tuned, ax=axes[0], importance_type='weight', max_num_features=10)\n",
    "axes[0].set_title('Feature Importance (Weight)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Gain importance\n",
    "xgb.plot_importance(xgb_tuned, ax=axes[1], importance_type='gain', max_num_features=10)\n",
    "axes[1].set_title('Feature Importance (Gain)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Cover importance\n",
    "xgb.plot_importance(xgb_tuned, ax=axes[2], importance_type='cover', max_num_features=10)\n",
    "axes[2].set_title('Feature Importance (Cover)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383569cf",
   "metadata": {},
   "source": [
    "## 13. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation on tuned model\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"10-FOLD CROSS-VALIDATION (Tuned Model)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ Running cross-validation...\")\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    xgb_tuned, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=10, \n",
    "    scoring=['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate RMSE from MSE\n",
    "cv_train_rmse = np.sqrt(-cv_results['train_neg_mean_squared_error'])\n",
    "cv_test_rmse = np.sqrt(-cv_results['test_neg_mean_squared_error'])\n",
    "\n",
    "print(f\"\\n‚úÖ Cross-validation completed!\")\n",
    "print(f\"\\nR¬≤ Score:\")\n",
    "print(f\"  Train: {cv_results['train_r2'].mean():.4f} (+/- {cv_results['train_r2'].std() * 2:.4f})\")\n",
    "print(f\"  Test:  {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std() * 2:.4f})\")\n",
    "print(f\"\\nRMSE:\")\n",
    "print(f\"  Train: {cv_train_rmse.mean():.4f} (+/- {cv_train_rmse.std() * 2:.4f})\")\n",
    "print(f\"  Test:  {cv_test_rmse.mean():.4f} (+/- {cv_test_rmse.std() * 2:.4f})\")\n",
    "print(f\"\\nMAE:\")\n",
    "print(f\"  Train: {-cv_results['train_neg_mean_absolute_error'].mean():.4f} (+/- {cv_results['train_neg_mean_absolute_error'].std() * 2:.4f})\")\n",
    "print(f\"  Test:  {-cv_results['test_neg_mean_absolute_error'].mean():.4f} (+/- {cv_results['test_neg_mean_absolute_error'].std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# R¬≤ scores\n",
    "axes[0].boxplot([cv_results['train_r2'], cv_results['test_r2']], \n",
    "                labels=['Train', 'Test'], patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'))\n",
    "axes[0].set_ylabel('R¬≤ Score', fontsize=11)\n",
    "axes[0].set_title('Cross-Validation R¬≤ Scores', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# RMSE\n",
    "axes[1].boxplot([cv_train_rmse, cv_test_rmse], \n",
    "                labels=['Train', 'Test'], patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightcoral'))\n",
    "axes[1].set_ylabel('RMSE', fontsize=11)\n",
    "axes[1].set_title('Cross-Validation RMSE', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# MAE\n",
    "axes[2].boxplot([-cv_results['train_neg_mean_absolute_error'], \n",
    "                 -cv_results['test_neg_mean_absolute_error']], \n",
    "                labels=['Train', 'Test'], patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightgreen'))\n",
    "axes[2].set_ylabel('MAE', fontsize=11)\n",
    "axes[2].set_title('Cross-Validation MAE', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee717e18",
   "metadata": {},
   "source": [
    "## 14. Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test = test_df.drop('Id', axis=1).copy()\n",
    "X_test['Lifestyle Activities'] = label_encoder.transform(X_test['Lifestyle Activities'])\n",
    "\n",
    "# Make predictions using tuned model\n",
    "test_predictions = xgb_tuned.predict(X_test)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST SET PREDICTIONS (Tuned XGBoost)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Predictions generated for {len(test_predictions)} test samples\")\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Mean: {test_predictions.mean():.2f}\")\n",
    "print(f\"  Std: {test_predictions.std():.2f}\")\n",
    "print(f\"  Min: {test_predictions.min():.2f}\")\n",
    "print(f\"  Max: {test_predictions.max():.2f}\")\n",
    "print(f\"  Median: {np.median(test_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(test_predictions, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(test_predictions.mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {test_predictions.mean():.2f}')\n",
    "axes[0].axvline(np.median(test_predictions), color='green', linestyle='--', \n",
    "               linewidth=2, label=f'Median: {np.median(test_predictions):.2f}')\n",
    "axes[0].set_xlabel('Predicted Recovery Index', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Test Predictions (Tuned XGBoost)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(test_predictions, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightpurple', color='purple'),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('Predicted Recovery Index', fontsize=12)\n",
    "axes[1].set_title('Box Plot of Test Predictions', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'Recovery Index': test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('xgboost_tuned_submission.csv', index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUBMISSION FILE CREATED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ File saved: xgboost_tuned_submission.csv\")\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "display(submission.head(10))\n",
    "print(f\"\\nLast 10 predictions:\")\n",
    "display(submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae608",
   "metadata": {},
   "source": [
    "## 15. Model Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a95d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBOOST MODEL - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä MODEL CONFIGURATION:\")\n",
    "print(f\"  Model Type: XGBoost Regressor\")\n",
    "print(f\"  Best Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"    - {param}: {value}\")\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE METRICS:\")\n",
    "print(f\"\\n  Baseline XGBoost:\")\n",
    "print(f\"    - Training R¬≤: {train_r2_baseline:.4f}\")\n",
    "print(f\"    - Validation R¬≤: {val_r2_baseline:.4f}\")\n",
    "print(f\"    - Validation RMSE: {val_rmse_baseline:.4f}\")\n",
    "print(f\"    - Validation MAE: {val_mae_baseline:.4f}\")\n",
    "\n",
    "print(f\"\\n  Tuned XGBoost:\")\n",
    "print(f\"    - Training R¬≤: {train_r2_tuned:.4f}\")\n",
    "print(f\"    - Validation R¬≤: {val_r2_tuned:.4f}\")\n",
    "print(f\"    - Validation RMSE: {val_rmse_tuned:.4f}\")\n",
    "print(f\"    - Validation MAE: {val_mae_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\n  10-Fold Cross-Validation (Tuned):\")\n",
    "print(f\"    - Mean Test R¬≤: {cv_results['test_r2'].mean():.4f} (+/- {cv_results['test_r2'].std() * 2:.4f})\")\n",
    "print(f\"    - Mean Test RMSE: {cv_test_rmse.mean():.4f} (+/- {cv_test_rmse.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nüîç TOP 3 MOST IMPORTANT FEATURES:\")\n",
    "for idx, row in feature_importance_tuned.head(3).iterrows():\n",
    "    print(f\"  {idx+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä TEST SET PREDICTIONS:\")\n",
    "print(f\"  Number of predictions: {len(test_predictions)}\")\n",
    "print(f\"  Prediction range: [{test_predictions.min():.2f}, {test_predictions.max():.2f}]\")\n",
    "print(f\"  Prediction mean: {test_predictions.mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511c707",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Key Findings & Next Steps\n",
    "\n",
    "### üéØ Key Findings\n",
    "\n",
    "1. **XGBoost Performance:**\n",
    "   - XGBoost typically outperforms Random Forest and Linear Regression\n",
    "   - Excellent handling of non-linear relationships\n",
    "   - Built-in regularization prevents overfitting\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - RandomizedSearchCV efficiently explores large parameter spaces\n",
    "   - Key parameters: learning_rate, max_depth, n_estimators\n",
    "   - Early stopping prevents overfitting\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - XGBoost provides multiple importance metrics (weight, gain, cover)\n",
    "   - Initial Health Score and Therapy Hours typically dominate\n",
    "   - Understanding feature interactions is crucial\n",
    "\n",
    "### üöÄ Next Steps for Further Improvement\n",
    "\n",
    "1. **Try Other Boosting Algorithms:**\n",
    "   - **LightGBM** (faster, histogram-based)\n",
    "   - **CatBoost** (handles categorical features natively)\n",
    "   - **Gradient Boosting** (sklearn implementation)\n",
    "\n",
    "2. **Advanced Feature Engineering:**\n",
    "   - Create interaction terms (e.g., `Therapy Hours √ó Initial Health Score`)\n",
    "   - Polynomial features of degree 2-3\n",
    "   - Ratio features (e.g., `Therapy Hours / Follow-Up Sessions`)\n",
    "   - Domain-specific transformations\n",
    "\n",
    "3. **Ensemble Strategies:**\n",
    "   - **Stacking**: Use XGBoost + Random Forest + Linear models\n",
    "   - **Voting**: Weight-averaged predictions\n",
    "   - **Blending**: Multiple model combinations\n",
    "\n",
    "4. **Further Hyperparameter Optimization:**\n",
    "   - Use **Optuna** or **Hyperopt** for Bayesian optimization\n",
    "   - Fine-tune learning rate schedule\n",
    "   - Experiment with different evaluation metrics\n",
    "\n",
    "5. **Model Analysis:**\n",
    "   - SHAP values for model interpretability\n",
    "   - Partial dependence plots\n",
    "   - Error analysis on specific data segments\n",
    "\n",
    "### üí° XGBoost Advantages\n",
    "\n",
    "- ‚úÖ **Speed**: Optimized C++ implementation\n",
    "- ‚úÖ **Performance**: State-of-the-art accuracy\n",
    "- ‚úÖ **Regularization**: Built-in L1/L2 regularization\n",
    "- ‚úÖ **Handling missing values**: Automatic handling\n",
    "- ‚úÖ **Feature importance**: Multiple importance metrics\n",
    "- ‚úÖ **Flexibility**: Extensive hyperparameter options\n",
    "\n",
    "### üìù Documentation Tips\n",
    "\n",
    "- Compare XGBoost with Random Forest and Linear Regression\n",
    "- Document the impact of different hyperparameters\n",
    "- Show learning curves and convergence\n",
    "- Explain feature importance insights\n",
    "- Discuss regularization benefits\n",
    "\n",
    "**Remember:** XGBoost is often a top performer in Kaggle competitions! Combine it with good feature engineering for best results. üèÜ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
